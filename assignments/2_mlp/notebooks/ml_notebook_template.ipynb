{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Notebook title -->\n",
    "# Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Notebook Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Task Description\n",
    "<!-- \n",
    "- A brief description of the problem you're solving with machine learning.\n",
    "- Define the objective (e.g., classification, regression, clustering, etc.).\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Useful Resources\n",
    "<!--\n",
    "- Links to relevant papers, articles, or documentation.\n",
    "- Description of the datasets (if external).\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1.1 Common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Datasets Kaggle](https://www.kaggle.com/datasets)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A vast repository of datasets across various domains provided by Kaggle, a platform for data science competitions.\n",
    "  \n",
    "* [Toy datasets from Sklearn](https://scikit-learn.org/stable/datasets/toy_dataset.html)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A collection of small datasets that come with the Scikit-learn library, useful for quick prototyping and testing algorithms.\n",
    "  \n",
    "* [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A widely-used repository for machine learning datasets, with a variety of real-world datasets available for research and experimentation.\n",
    "  \n",
    "* [Google Dataset Search](https://datasetsearch.research.google.com/)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A tool from Google that helps to find datasets stored across the web, with a focus on publicly available data.\n",
    "  \n",
    "* [AWS Public Datasets](https://registry.opendata.aws/)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A registry of publicly available datasets that can be analyzed on the cloud using Amazon Web Services (AWS).\n",
    "  \n",
    "* [Microsoft Azure Open Datasets](https://azure.microsoft.com/en-us/services/open-datasets/)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A collection of curated datasets from various domains, made available by Microsoft Azure for use in machine learning and analytics.\n",
    "  \n",
    "* [Awesome Public Datasets](https://github.com/awesomedata/awesome-public-datasets)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A GitHub repository that lists a wide variety of datasets across different domains, curated by the community.\n",
    "  \n",
    "* [Data.gov](https://www.data.gov/)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A portal to the US government's open data, offering access to a wide range of datasets from various federal agencies.\n",
    "  \n",
    "* [Google BigQuery Public Datasets](https://cloud.google.com/bigquery/public-data)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;Public datasets hosted by Google BigQuery, allowing for quick and powerful querying of large datasets in the cloud.\n",
    "  \n",
    "* [Papers with Code](https://paperswithcode.com/datasets)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A platform that links research papers with the corresponding code and datasets, helping researchers reproduce results and explore new data.\n",
    "  \n",
    "* [Zenodo](https://zenodo.org/)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;An open-access repository that allows researchers to share datasets, software, and other research outputs, often linked to academic publications.\n",
    "  \n",
    "* [The World Bank Open Data](https://data.worldbank.org/)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A comprehensive source of global development data, with datasets covering various economic and social indicators.\n",
    "  \n",
    "* [OpenML](https://www.openml.org/)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;An online platform for sharing datasets, machine learning experiments, and results, fostering collaboration in the ML community.\n",
    "  \n",
    "* [Stanford Large Network Dataset Collection (SNAP)](https://snap.stanford.edu/data/)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A collection of large-scale network datasets from Stanford University, useful for network analysis and graph-based machine learning.\n",
    "  \n",
    "* [KDnuggets Datasets](https://www.kdnuggets.com/datasets/index.html)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A curated list of datasets for data mining and data science, compiled by the KDnuggets community.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1.2 Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [K-Nearest Neighbors on Kaggle](https://www.kaggle.com/code/mmdatainfo/k-nearest-neighbors)\n",
    "\n",
    "* [Complete Guide to K-Nearest-Neighbors](https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Imports\n",
    "<!--\n",
    "- Import necessary libraries (e.g., `numpy`, `pandas`, `matplotlib`, `scikit-learn`, etc.).\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ikt450.src.common_imports import *\n",
    "from ikt450.src.config import get_paths\n",
    "from ikt450.src.common_func import load_dataset, save_dataframe, ensure_dir_exists\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Global Variables\n",
    "<!--\n",
    "- Define global constants, paths, and configuration settings used throughout the notebook.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = get_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Split ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLITRATIO = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Function Definitions\n",
    "<!--\n",
    "- Define helper functions that will be used multiple times in the notebook.\n",
    "- Consider organizing these into separate sections (e.g., data processing functions, model evaluation functions).\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. System Setup \n",
    "<!-- (Optional but recommended) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Styling\n",
    "<!--\n",
    "- Set up any visual styles (e.g., for plots).\n",
    "- Configure notebook display settings (e.g., `matplotlib` defaults, pandas display options).\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Environment Configuration\n",
    "<!--\n",
    "- Check system dependencies, versions, and ensure reproducibility (e.g., set random seeds).\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Data loading\n",
    "<!--\n",
    "- Load datasets from files or other sources.\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls {paths['PATH_COMMON_DATASETS']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{paths['PATH_COMMON_DATASETS']}/ecoli.data\",  delim_whitespace=True, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Data inspection\n",
    "<!--\n",
    "- Preview the data (e.g., `head`, `describe`).\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Add code for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Data Cleaning\n",
    "<!--\n",
    "- Handle missing values, outliers, and inconsistencies.\n",
    "- Remove or impute missing data.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 NULL, NaN, Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Feature Engineering\n",
    "<!--\n",
    "- Create new features from existing data.\n",
    "- Normalize or standardize features.\n",
    "- Encode categorical variables.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1 Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.1.1 Feature Selection / Data Separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<br>\n",
    "<details>\n",
    "<summary>What does it?</summary>\n",
    "<br>\n",
    "This line removes the `` column from the DataFrame `df` and assigns the remaining columns to `X`.\n",
    "</details>\n",
    "<br>\n",
    "<details>\n",
    "<summary>Why do we do it?</summary>\n",
    "<br>\n",
    "We do this to separate the input features (which are stored in `X`) from the target variable (which will be stored in `y`). This separation is essential in supervised learning tasks where the goal is to predict the target variable based on the input features.\n",
    "</details>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={0: 'id', 1: 'mcg', 2: 'gvh', 3: 'lip', 4: 'chg', 5: 'aac', 6: 'alm1', 7: 'alm2', 8: 'class'}, inplace=True)\n",
    "\n",
    "# remove id column\n",
    "df.drop('id', axis=1, inplace=True)\n",
    "\n",
    "# keep only the rows with class 'cp' or 'im'\n",
    "df = df[df['class'].isin(['cp', 'im'])]\n",
    "# remove column chg\n",
    "df.drop('chg', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# encode class\n",
    "df['class'] = df['class'].map({'cp': 0, 'im': 1})\n",
    "\n",
    "# split data into X and y\n",
    "X_data = df.drop('class', axis=1)\n",
    "Y_data = df['class']\n",
    "\n",
    "# standardize the data\n",
    "X_data = (X_data - X_data.mean()) / X_data.std()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.1.2 Target Variable Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<br>\n",
    "<details>\n",
    "<summary>What does it?</summary>\n",
    "<br>\n",
    "This line selects the `` column from the DataFrame `df` and assigns it to `y`.\n",
    "</details>\n",
    "<br>\n",
    "<details>\n",
    "<summary>Why do we do it?</summary>\n",
    "<br>\n",
    "We do this to isolate the target variable, which represents the labels or outcomes that we aim to predict using the machine learning model.\n",
    "</details>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class preseptron():\n",
    "    def __init__(self, n_inputs):\n",
    "        self.w = np.random.rand(n_inputs)*0.001\n",
    "        self.b = np.random.rand(1)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        x = np.clip(x, -500, 500)  # Clipping to prevent overflow\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "     \n",
    "        \n",
    "        self.latest_output = self.sigmoid(np.dot(x, self.w) + self.b)\n",
    "        self.latest_input = x\n",
    "        return self.latest_output\n",
    "    \n",
    "    def backward(self,error,lr):\n",
    "\n",
    "        gradient = error * self.sigmoid_derivative(self.latest_output)\n",
    " \n",
    "        self.w += lr * gradient * self.latest_input\n",
    "        self.b += lr * gradient\n",
    "        return gradient * self.w\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "def mse (y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_scrach():\n",
    "    def __init__(self, layer1_size, layer2_size, layer3_size, lr=0.1):\n",
    "        self.lr = lr\n",
    "        self.layer1 = [preseptron(6) for _ in range(layer1_size)]\n",
    "        self.layer2 = [preseptron(layer1_size) for _ in range(layer2_size)]\n",
    "        self.layer3 = [preseptron(layer2_size) for _ in range(layer3_size)]\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        inputs = np.array(inputs)\n",
    "        \n",
    "        x = []\n",
    "        for l in self.layer1:\n",
    "            x.append(l.forward(inputs))\n",
    "        inputs = np.array(x)\n",
    "        inputs = np.reshape(inputs, (len(inputs)))\n",
    "        \n",
    "        x = []\n",
    "        for l in self.layer2:\n",
    "            x.append(l.forward(inputs))\n",
    "        inputs = np.array(x)\n",
    "        inputs = np.reshape(inputs, (len(inputs)))\n",
    "        x = []\n",
    "        for l in self.layer3:\n",
    "            x.append(l.forward(inputs))\n",
    "        return np.array(x)\n",
    "    \n",
    "    def backward(self,error):\n",
    "        initial_error = error\n",
    "        new_error = []\n",
    "        for l, err in zip(self.layer3,initial_error):\n",
    "            new_error.append(l.backward(err,self.lr))\n",
    "        initial_error = np.array(new_error)\n",
    "        initial_error = np.sum(initial_error, axis=0)\n",
    "        new_error = []\n",
    "        for l, err in zip(self.layer2,initial_error):\n",
    "            new_error.append(l.backward(err,self.lr))\n",
    "        initial_error = np.array(new_error)\n",
    "  \n",
    "        initial_error = np.sum(initial_error, axis=0)\n",
    "    \n",
    "        new_error = []\n",
    "        for l, err in zip(self.layer1,initial_error):\n",
    "            new_error.append(l.backward(err,self.lr))\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try to train the model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "model = MLP_scrach(5,5,1, 0.05)\n",
    "# split the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, Y_data, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "# train the model\n",
    "\n",
    "for i in range(1000):\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # shuffle the training data\n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "    for x,y in zip(X_train.values, y_train.values):\n",
    "       \n",
    "        y_pred = model.forward(x)\n",
    "        \n",
    "        error = y - y_pred\n",
    "        epoch_loss += mse(y, y_pred)\n",
    "       \n",
    "        model.backward(error)\n",
    "    # print the loss every 100 epochs 313 \n",
    "    if i % 100 == 0:\n",
    "\n",
    "        print(f\"Epoch {i} loss: {epoch_loss}\")\n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test the model\n",
    "y_pred = []\n",
    "for x in X_test.values:\n",
    "    y_pred.append(model.forward(x))\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred = np.reshape(y_pred, (len(y_pred)))\n",
    "print(mse(y_test.values, y_pred))\n",
    "accuracy = np.mean(y_test.values == np.round(y_pred))\n",
    "accuracy\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Model Selection\n",
    "<!--\n",
    "- Choose the model(s) to be trained (e.g., linear regression, decision trees, neural networks).\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Model Training\n",
    "<!--\n",
    "- Train the selected model(s) using the training data.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Model Evaluation\n",
    "<!--\n",
    "- Evaluate model performance on validation data.\n",
    "- Use appropriate metrics (e.g., accuracy, precision, recall, RMSE).\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Hyperparameter Tuning\n",
    "<!--\n",
    "- Fine-tune the model using techniques like Grid Search or Random Search.\n",
    "- Evaluate the impact of different hyperparameters.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Model Testing\n",
    "<!--\n",
    "- Evaluate the final model on the test dataset.\n",
    "- Ensure that the model generalizes well to unseen data.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Model Interpretation (Optional)\n",
    "<!--\n",
    "- Interpret the model results (e.g., feature importance, SHAP values).\n",
    "- Discuss the strengths and limitations of the model.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Make Predictions\n",
    "<!--\n",
    "- Use the trained model to make predictions on new/unseen data.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Save Model and Results\n",
    "<!--\n",
    "- Save the trained model to disk for future use.\n",
    "- Export prediction results for further analysis.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Documentation and Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Summary of Findings\n",
    "<!--\n",
    "- Summarize the results and findings of the analysis.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Next Steps\n",
    "<!--\n",
    "- Suggest further improvements, alternative models, or future work.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 References\n",
    "<!--\n",
    "- Cite any resources, papers, or documentation used.\n",
    "-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
