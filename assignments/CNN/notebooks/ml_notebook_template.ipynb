{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Notebook title -->\n",
    "# Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Notebook Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Task Description\n",
    "<!-- \n",
    "- A brief description of the problem you're solving with machine learning.\n",
    "- Define the objective (e.g., classification, regression, clustering, etc.).\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Useful Resources\n",
    "<!--\n",
    "- Links to relevant papers, articles, or documentation.\n",
    "- Description of the datasets (if external).\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1.1 Common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Datasets Kaggle](https://www.kaggle.com/datasets)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A vast repository of datasets across various domains provided by Kaggle, a platform for data science competitions.\n",
    "  \n",
    "* [Toy datasets from Sklearn](https://scikit-learn.org/stable/datasets/toy_dataset.html)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A collection of small datasets that come with the Scikit-learn library, useful for quick prototyping and testing algorithms.\n",
    "  \n",
    "* [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A widely-used repository for machine learning datasets, with a variety of real-world datasets available for research and experimentation.\n",
    "  \n",
    "* [Google Dataset Search](https://datasetsearch.research.google.com/)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A tool from Google that helps to find datasets stored across the web, with a focus on publicly available data.\n",
    "  \n",
    "* [AWS Public Datasets](https://registry.opendata.aws/)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A registry of publicly available datasets that can be analyzed on the cloud using Amazon Web Services (AWS).\n",
    "  \n",
    "* [Microsoft Azure Open Datasets](https://azure.microsoft.com/en-us/services/open-datasets/)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A collection of curated datasets from various domains, made available by Microsoft Azure for use in machine learning and analytics.\n",
    "  \n",
    "* [Awesome Public Datasets](https://github.com/awesomedata/awesome-public-datasets)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A GitHub repository that lists a wide variety of datasets across different domains, curated by the community.\n",
    "  \n",
    "* [Data.gov](https://www.data.gov/)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A portal to the US government's open data, offering access to a wide range of datasets from various federal agencies.\n",
    "  \n",
    "* [Google BigQuery Public Datasets](https://cloud.google.com/bigquery/public-data)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;Public datasets hosted by Google BigQuery, allowing for quick and powerful querying of large datasets in the cloud.\n",
    "  \n",
    "* [Papers with Code](https://paperswithcode.com/datasets)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A platform that links research papers with the corresponding code and datasets, helping researchers reproduce results and explore new data.\n",
    "  \n",
    "* [Zenodo](https://zenodo.org/)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;An open-access repository that allows researchers to share datasets, software, and other research outputs, often linked to academic publications.\n",
    "  \n",
    "* [The World Bank Open Data](https://data.worldbank.org/)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A comprehensive source of global development data, with datasets covering various economic and social indicators.\n",
    "  \n",
    "* [OpenML](https://www.openml.org/)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;An online platform for sharing datasets, machine learning experiments, and results, fostering collaboration in the ML community.\n",
    "  \n",
    "* [Stanford Large Network Dataset Collection (SNAP)](https://snap.stanford.edu/data/)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A collection of large-scale network datasets from Stanford University, useful for network analysis and graph-based machine learning.\n",
    "  \n",
    "* [KDnuggets Datasets](https://www.kdnuggets.com/datasets/index.html)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A curated list of datasets for data mining and data science, compiled by the KDnuggets community.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1.2 Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [K-Nearest Neighbors on Kaggle](https://www.kaggle.com/code/mmdatainfo/k-nearest-neighbors)\n",
    "\n",
    "* [Complete Guide to K-Nearest-Neighbors](https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Imports\n",
    "<!--\n",
    "- Import necessary libraries (e.g., `numpy`, `pandas`, `matplotlib`, `scikit-learn`, etc.).\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torchsummary -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ikt450.src.common_imports import *\n",
    "from ikt450.src.config import get_paths\n",
    "from ikt450.src.common_func import load_dataset, save_dataframe, ensure_dir_exists\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if 0 else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Global Variables\n",
    "<!--\n",
    "- Define global constants, paths, and configuration settings used throughout the notebook.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = get_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Split ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLITRATIO = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PATH_PROJECT_ROOT': 'C:\\\\Users\\\\jonin\\\\Documents\\\\ikt450\\\\ikt450',\n",
       " 'PATH_ASSIGNMENTS': 'C:\\\\Users\\\\jonin\\\\Documents\\\\ikt450\\\\ikt450\\\\assignments',\n",
       " 'PATH_COMMON': 'C:\\\\Users\\\\jonin\\\\Documents\\\\ikt450\\\\ikt450\\\\common',\n",
       " 'PATH_COMMON_DATASETS': 'C:\\\\Users\\\\jonin\\\\Documents\\\\ikt450\\\\ikt450\\\\common\\\\datasets',\n",
       " 'PATH_COMMON_NOTEBOOKS': 'C:\\\\Users\\\\jonin\\\\Documents\\\\ikt450\\\\ikt450\\\\common\\\\notebooks',\n",
       " 'PATH_COMMON_RESOURCES': 'C:\\\\Users\\\\jonin\\\\Documents\\\\ikt450\\\\ikt450\\\\common\\\\resources',\n",
       " 'PATH_COMMON_SCRIPTS': 'C:\\\\Users\\\\jonin\\\\Documents\\\\ikt450\\\\ikt450\\\\common\\\\scripts',\n",
       " 'PATH_REPORTS': 'C:\\\\Users\\\\jonin\\\\Documents\\\\ikt450\\\\ikt450\\\\reports',\n",
       " 'PATH_SRC': 'C:\\\\Users\\\\jonin\\\\Documents\\\\ikt450\\\\ikt450\\\\src',\n",
       " 'PATH_1_KNN': 'C:\\\\Users\\\\jonin\\\\Documents\\\\ikt450\\\\ikt450\\\\assignments\\\\1_knn',\n",
       " 'PATH_2_MLP': 'C:\\\\Users\\\\jonin\\\\Documents\\\\ikt450\\\\ikt450\\\\assignments\\\\2_mlp',\n",
       " 'PATH_CNN': 'C:\\\\Users\\\\jonin\\\\Documents\\\\ikt450\\\\ikt450\\\\assignments\\\\CNN'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([0.5548, 0.4508, 0.3435])\n",
      "Std: tensor([0.2280, 0.2384, 0.2374])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Temporary transform to convert images to tensors\n",
    "temp_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root=f\"{paths['PATH_COMMON_DATASETS']}/food11/training\", transform=temp_transform)\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "mean = 0.\n",
    "std = 0.\n",
    "total_images_count = 0\n",
    "\n",
    "for images, _ in loader:\n",
    "    batch_samples = images.size(0)\n",
    "    images = images.view(batch_samples, images.size(1), -1)\n",
    "    mean += images.mean(2).sum(0)\n",
    "    std += images.std(2).sum(0)\n",
    "    total_images_count += batch_samples\n",
    "\n",
    "mean /= total_images_count\n",
    "std /= total_images_count\n",
    "\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Std: {std}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 3430\n",
       "    Root location: C:\\Users\\jonin\\Documents\\ikt450\\ikt450\\common\\datasets/food11/validation\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
       "               ToTensor()\n",
       "               Normalize(mean=[0.554803729057312, 0.45082226395606995, 0.343528687953949], std=[0.2280411571264267, 0.23836620151996613, 0.2374396026134491])\n",
       "           )"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset Food11 from PATH_COMMON_DATASETS/food11\n",
    "# the folder structure is PATH_COMMON_DATASETS/food11/training and PATH_COMMON_DATASETS/food11/evaluation and PATH_COMMON_DATASETS/food11/validation\n",
    "#  and in these folders there are 11 subfolders with the class names\n",
    "#  and in these subfolders there are the images\n",
    "#  the dataset is loaded with the torchvision.datasets.ImageFolder function\n",
    "#  and the images are transformed to tensors and normalized\n",
    "\n",
    "# find the mean and std of the dataset at paths['PATH_COMMON_DATASETS']}/food11/training\n",
    "#  and use these values to normalize the images\n",
    "#  the values are found with the function get_mean_std\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), # random values from copilot\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean.tolist(), std=std.tolist()) # random values from copilot\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder(root=f\"{paths['PATH_COMMON_DATASETS']}/food11/training\" , transform=transform)\n",
    "train_dataset\n",
    "test_dataset = torchvision.datasets.ImageFolder(root=f\"{paths['PATH_COMMON_DATASETS']}/food11/evaluation\" , transform=transform)\n",
    "test_dataset\n",
    "val_dataset = torchvision.datasets.ImageFolder(root=f\"{paths['PATH_COMMON_DATASETS']}/food11/validation\" , transform=transform)\n",
    "val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 994,\n",
       " 1: 429,\n",
       " 2: 1500,\n",
       " 3: 986,\n",
       " 4: 848,\n",
       " 5: 1325,\n",
       " 6: 440,\n",
       " 7: 280,\n",
       " 8: 855,\n",
       " 9: 1500,\n",
       " 10: 709}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at label distribution\n",
    "# sum up the number of images in each class\n",
    "\n",
    "label_count = {}\n",
    "for i in train_dataset.targets:\n",
    "    if i in label_count:\n",
    "        label_count[i] += 1\n",
    "    else:\n",
    "        label_count[i] = 1\n",
    "label_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Function Definitions\n",
    "<!--\n",
    "- Define helper functions that will be used multiple times in the notebook.\n",
    "- Consider organizing these into separate sections (e.g., data processing functions, model evaluation functions).\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Model Selection\n",
    "<!--\n",
    "- Choose the model(s) to be trained (e.g., linear regression, decision trees, neural networks).\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        \n",
    "        # First convolutional layer (input channels=3 for RGB, output channels=6, kernel size=5)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5)\n",
    "        \n",
    "        # Second convolutional layer (input channels=6, output channels=16, kernel size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
    "        \n",
    "        # After two conv + pooling layers, the feature map size will be reduced.\n",
    "        # For an input image of size 224x224, the final feature map size after conv and pooling is 53x53.\n",
    "        # 16 * 53 * 53 = 44944 flattened features going into the first fully connected layer.\n",
    "        self.fc1 = nn.Linear(16 * 53 * 53, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 11)  # Output layer for classification (e.g., 10 classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First convolution, ReLU, and max-pooling layer\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), kernel_size=2, stride=2)  # Output size: (6, 110, 110)\n",
    "        \n",
    "        # Second convolution, ReLU, and max-pooling layer\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), kernel_size=2, stride=2)  # Output size: (16, 53, 53)\n",
    "        \n",
    "        # Flatten the feature maps for the fully connected layers\n",
    "        x = x.view(-1, 16 * 53 * 53)  # Reshape to (batch_size, 16 * 53 * 53)\n",
    "        \n",
    "        # Fully connected layers with ReLU activation\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        # Output layer (raw scores for classification)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Model Training\n",
    "<!--\n",
    "- Train the selected model(s) using the training data.\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet()\n",
    "\n",
    "model.to(device)\n",
    "model\n",
    "from torchsummary import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define the loss function and the optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00003, weight_decay=0.0001)\n",
    "\n",
    "n_epochs = 50\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loss: 2.249315782999381 val_loss: 2.180709883018776\n",
      "2 loss: 2.1235830860260205 val_loss: 2.072818285889096\n",
      "3 loss: 1.9961094840978966 val_loss: 1.9843501801843997\n",
      "4 loss: 1.9013986939038985 val_loss: 1.9231588134059199\n",
      "5 loss: 1.838922714575743 val_loss: 1.9002644574200664\n",
      "6 loss: 1.78883800139794 val_loss: 1.87499232645388\n",
      "7 loss: 1.7497532337139814 val_loss: 1.8483291886470936\n",
      "8 loss: 1.7136274698453071 val_loss: 1.8400681482421026\n",
      "9 loss: 1.6846596919573271 val_loss: 1.8349433143933613\n",
      "10 loss: 1.654812828088418 val_loss: 1.8201173080338373\n",
      "11 loss: 1.6228799132200389 val_loss: 1.8121119605170355\n",
      "12 loss: 1.5854855989798522 val_loss: 1.8204928261262399\n",
      "13 loss: 1.5690786364750984 val_loss: 1.8072326337849651\n",
      "14 loss: 1.5391978911864452 val_loss: 1.7951082962530631\n",
      "15 loss: 1.5169228055538275 val_loss: 1.78363616598977\n",
      "16 loss: 1.4919580603257203 val_loss: 1.7950693236456976\n",
      "17 loss: 1.4780495350177472 val_loss: 1.7839141841287967\n",
      "18 loss: 1.4492293856082819 val_loss: 1.8164138705642134\n",
      "19 loss: 1.4255487674321883 val_loss: 1.7989243335194058\n",
      "20 loss: 1.4051665434470544 val_loss: 1.8062507112820942\n",
      "21 loss: 1.3634012586031206 val_loss: 1.789448133221379\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        for i, data in enumerate(val_loader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "    print(f\"{epoch + 1} loss: {running_loss / len(train_loader)} val_loss: {val_loss / len(val_loader)}\")\n",
    "\n",
    "    running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 2.096067282888624\n",
      "Validation accuracy: 0.3498542274052478\n",
      "Validation f1: 0.31352571673170676\n",
      "Validation precision: 0.33166029471446107\n",
      "Validation recall: 0.3154993137147597\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "          Bread     0.2176    0.2873    0.2476       362\n",
      "  Dairy product     0.1954    0.2361    0.2138       144\n",
      "        Dessert     0.3434    0.2960    0.3179       500\n",
      "            Egg     0.3307    0.2599    0.2911       327\n",
      "     Fried food     0.2857    0.0982    0.1461       326\n",
      "           Meat     0.3501    0.5880    0.4389       449\n",
      "  Noodles-Pasta     0.2808    0.2789    0.2799       147\n",
      "           Rice     0.2400    0.1250    0.1644        96\n",
      "        Seafood     0.2799    0.2767    0.2783       347\n",
      "           Soup     0.5583    0.5460    0.5521       500\n",
      "Vegetable-Fruit     0.5663    0.4784    0.5187       232\n",
      "\n",
      "       accuracy                         0.3499      3430\n",
      "      macro avg     0.3317    0.3155    0.3135      3430\n",
      "   weighted avg     0.3525    0.3499    0.3409      3430\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,f1_score, precision_score, recall_score\n",
    "val_loss = 0.0\n",
    "val_labels = []\n",
    "val_preds = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in val_loader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        val_loss += loss.item()\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        val_labels += labels.cpu().numpy().tolist()\n",
    "        val_preds += preds.cpu().numpy().tolist()\n",
    "\n",
    "print(f\"Validation loss: {val_loss / len(val_loader)}\")\n",
    "print(f\"Validation accuracy: {accuracy_score(val_labels, val_preds)}\")\n",
    "print(f\"Validation f1: {f1_score(val_labels, val_preds, average='macro')}\")\n",
    "print(f\"Validation precision: {precision_score(val_labels, val_preds, average='macro')}\")\n",
    "print(f\"Validation recall: {recall_score(val_labels, val_preds, average='macro')}\")\n",
    "print(classification_report(val_labels, val_preds, target_names=train_dataset.classes,digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=11, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import resnet18\n",
    "import torchvision.models as models\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "resnet18.fc = nn.Linear(resnet18.fc.in_features, 11)\n",
    "resnet18.to(device)\n",
    "resnet18\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00003, weight_decay=0.0001)\n",
    "\n",
    "n_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loss: 2.274372941408402 val_loss: 2.201375764829141\n",
      "2 loss: 2.130312604781909 val_loss: 2.0970189659683793\n",
      "3 loss: 2.0331082130089784 val_loss: 2.0277231953762196\n",
      "4 loss: 1.9831624978627913 val_loss: 1.9999745907606903\n",
      "5 loss: 1.9261883084590619 val_loss: 1.9614045178448711\n",
      "6 loss: 1.8971231411664913 val_loss: 1.9448033836152818\n",
      "7 loss: 1.855552891890208 val_loss: 1.929646540571142\n",
      "8 loss: 1.8196741969157488 val_loss: 1.9065444822664614\n",
      "9 loss: 1.784953128068875 val_loss: 1.895694993160389\n",
      "10 loss: 1.7550188241860805 val_loss: 1.8708153896861606\n",
      "11 loss: 1.7240770168793507 val_loss: 1.857690433661143\n",
      "12 loss: 1.6955128526076293 val_loss: 1.8449902269575331\n",
      "13 loss: 1.6647130128664849 val_loss: 1.8476652878302116\n",
      "14 loss: 1.646790855970138 val_loss: 1.8202971109637507\n",
      "15 loss: 1.6063159536092708 val_loss: 1.8087564287362274\n",
      "16 loss: 1.5870464535859914 val_loss: 1.8064837919341192\n",
      "17 loss: 1.5623165323184087 val_loss: 1.8186772730615404\n",
      "18 loss: 1.5403590080065606 val_loss: 1.8095576719001487\n",
      "19 loss: 1.5190076308372693 val_loss: 1.8200980248274627\n",
      "20 loss: 1.4937738012044857 val_loss: 1.7897911270459492\n",
      "21 loss: 1.4653634581810389 val_loss: 1.7849837607807584\n",
      "22 loss: 1.4467357534628649 val_loss: 1.77897212461189\n",
      "23 loss: 1.427737479026501 val_loss: 1.8028098256499678\n",
      "24 loss: 1.4076588597053137 val_loss: 1.8001727682572823\n",
      "25 loss: 1.379580985277127 val_loss: 1.7845903436342876\n",
      "26 loss: 1.3547354264137073 val_loss: 1.802805651117254\n",
      "27 loss: 1.344279176149613 val_loss: 1.812332970124704\n",
      "28 loss: 1.3250402945738573 val_loss: 1.806014895439148\n",
      "29 loss: 1.2986191312472026 val_loss: 1.7957142966764945\n",
      "30 loss: 1.2697263405873225 val_loss: 1.8172550974068817\n",
      "31 loss: 1.256760482604687 val_loss: 1.8043617330215596\n",
      "32 loss: 1.2275523711473515 val_loss: 1.7927122723173212\n",
      "33 loss: 1.204343861494309 val_loss: 1.8119359502085932\n",
      "34 loss: 1.1894187025534801 val_loss: 1.8045193950335185\n",
      "35 loss: 1.1654731531937916 val_loss: 1.8108361528979406\n",
      "36 loss: 1.1378053411459312 val_loss: 1.8157500933717798\n",
      "37 loss: 1.1237188287270374 val_loss: 1.8692643830069788\n",
      "38 loss: 1.1206219318585517 val_loss: 1.8211964114948556\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        for i, data in enumerate(val_loader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "    print(f\"{epoch + 1} loss: {running_loss / len(train_loader)} val_loss: {val_loss / len(val_loader)}\")\n",
    "\n",
    "    running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Model Evaluation\n",
    "<!--\n",
    "- Evaluate model performance on validation data.\n",
    "- Use appropriate metrics (e.g., accuracy, precision, recall, RMSE).\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Hyperparameter Tuning\n",
    "<!--\n",
    "- Fine-tune the model using techniques like Grid Search or Random Search.\n",
    "- Evaluate the impact of different hyperparameters.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Model Testing\n",
    "<!--\n",
    "- Evaluate the final model on the test dataset.\n",
    "- Ensure that the model generalizes well to unseen data.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Model Interpretation (Optional)\n",
    "<!--\n",
    "- Interpret the model results (e.g., feature importance, SHAP values).\n",
    "- Discuss the strengths and limitations of the model.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Make Predictions\n",
    "<!--\n",
    "- Use the trained model to make predictions on new/unseen data.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Save Model and Results\n",
    "<!--\n",
    "- Save the trained model to disk for future use.\n",
    "- Export prediction results for further analysis.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Documentation and Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Summary of Findings\n",
    "<!--\n",
    "- Summarize the results and findings of the analysis.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Next Steps\n",
    "<!--\n",
    "- Suggest further improvements, alternative models, or future work.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 References\n",
    "<!--\n",
    "- Cite any resources, papers, or documentation used.\n",
    "-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
