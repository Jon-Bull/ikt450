{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Notebook title -->\n",
    "# Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Notebook Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Task Description\n",
    "<!-- \n",
    "- A brief description of the problem you're solving with machine learning.\n",
    "- Define the objective (e.g., classification, regression, clustering, etc.).\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Useful Resources\n",
    "<!--\n",
    "- Links to relevant papers, articles, or documentation.\n",
    "- Description of the datasets (if external).\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1.1 Common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Datasets Kaggle](https://www.kaggle.com/datasets)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A vast repository of datasets across various domains provided by Kaggle, a platform for data science competitions.\n",
    "  \n",
    "* [Toy datasets from Sklearn](https://scikit-learn.org/stable/datasets/toy_dataset.html)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A collection of small datasets that come with the Scikit-learn library, useful for quick prototyping and testing algorithms.\n",
    "  \n",
    "* [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A widely-used repository for machine learning datasets, with a variety of real-world datasets available for research and experimentation.\n",
    "  \n",
    "* [Google Dataset Search](https://datasetsearch.research.google.com/)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A tool from Google that helps to find datasets stored across the web, with a focus on publicly available data.\n",
    "  \n",
    "* [AWS Public Datasets](https://registry.opendata.aws/)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A registry of publicly available datasets that can be analyzed on the cloud using Amazon Web Services (AWS).\n",
    "  \n",
    "* [Microsoft Azure Open Datasets](https://azure.microsoft.com/en-us/services/open-datasets/)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A collection of curated datasets from various domains, made available by Microsoft Azure for use in machine learning and analytics.\n",
    "  \n",
    "* [Awesome Public Datasets](https://github.com/awesomedata/awesome-public-datasets)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A GitHub repository that lists a wide variety of datasets across different domains, curated by the community.\n",
    "  \n",
    "* [Data.gov](https://www.data.gov/)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A portal to the US government's open data, offering access to a wide range of datasets from various federal agencies.\n",
    "  \n",
    "* [Google BigQuery Public Datasets](https://cloud.google.com/bigquery/public-data)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;Public datasets hosted by Google BigQuery, allowing for quick and powerful querying of large datasets in the cloud.\n",
    "  \n",
    "* [Papers with Code](https://paperswithcode.com/datasets)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A platform that links research papers with the corresponding code and datasets, helping researchers reproduce results and explore new data.\n",
    "  \n",
    "* [Zenodo](https://zenodo.org/)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;An open-access repository that allows researchers to share datasets, software, and other research outputs, often linked to academic publications.\n",
    "  \n",
    "* [The World Bank Open Data](https://data.worldbank.org/)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A comprehensive source of global development data, with datasets covering various economic and social indicators.\n",
    "  \n",
    "* [OpenML](https://www.openml.org/)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;An online platform for sharing datasets, machine learning experiments, and results, fostering collaboration in the ML community.\n",
    "  \n",
    "* [Stanford Large Network Dataset Collection (SNAP)](https://snap.stanford.edu/data/)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A collection of large-scale network datasets from Stanford University, useful for network analysis and graph-based machine learning.\n",
    "  \n",
    "* [KDnuggets Datasets](https://www.kdnuggets.com/datasets/index.html)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A curated list of datasets for data mining and data science, compiled by the KDnuggets community.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1.2 Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [K-Nearest Neighbors on Kaggle](https://www.kaggle.com/code/mmdatainfo/k-nearest-neighbors)\n",
    "\n",
    "* [Complete Guide to K-Nearest-Neighbors](https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Imports\n",
    "<!--\n",
    "- Import necessary libraries (e.g., `numpy`, `pandas`, `matplotlib`, `scikit-learn`, etc.).\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ikt450.src.common_imports import *\n",
    "from ikt450.src.config import get_paths\n",
    "from ikt450.src.common_func import load_dataset, save_dataframe, ensure_dir_exists\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Global Variables\n",
    "<!--\n",
    "- Define global constants, paths, and configuration settings used throughout the notebook.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = get_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Split ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLITRATIO = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PATH_PROJECT_ROOT': 'C:\\\\Users\\\\jonin\\\\Documents\\\\ikt450\\\\ikt450',\n",
       " 'PATH_ASSIGNMENTS': 'C:\\\\Users\\\\jonin\\\\Documents\\\\ikt450\\\\ikt450\\\\assignments',\n",
       " 'PATH_COMMON': 'C:\\\\Users\\\\jonin\\\\Documents\\\\ikt450\\\\ikt450\\\\common',\n",
       " 'PATH_COMMON_DATASETS': 'C:\\\\Users\\\\jonin\\\\Documents\\\\ikt450\\\\ikt450\\\\common\\\\datasets',\n",
       " 'PATH_COMMON_NOTEBOOKS': 'C:\\\\Users\\\\jonin\\\\Documents\\\\ikt450\\\\ikt450\\\\common\\\\notebooks',\n",
       " 'PATH_COMMON_RESOURCES': 'C:\\\\Users\\\\jonin\\\\Documents\\\\ikt450\\\\ikt450\\\\common\\\\resources',\n",
       " 'PATH_COMMON_SCRIPTS': 'C:\\\\Users\\\\jonin\\\\Documents\\\\ikt450\\\\ikt450\\\\common\\\\scripts',\n",
       " 'PATH_REPORTS': 'C:\\\\Users\\\\jonin\\\\Documents\\\\ikt450\\\\ikt450\\\\reports',\n",
       " 'PATH_SRC': 'C:\\\\Users\\\\jonin\\\\Documents\\\\ikt450\\\\ikt450\\\\src',\n",
       " 'PATH_1_KNN': 'C:\\\\Users\\\\jonin\\\\Documents\\\\ikt450\\\\ikt450\\\\assignments\\\\1_knn',\n",
       " 'PATH_2_MLP': 'C:\\\\Users\\\\jonin\\\\Documents\\\\ikt450\\\\ikt450\\\\assignments\\\\2_mlp',\n",
       " 'PATH_CNN': 'C:\\\\Users\\\\jonin\\\\Documents\\\\ikt450\\\\ikt450\\\\assignments\\\\CNN'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 3430\n",
       "    Root location: C:\\Users\\jonin\\Documents\\ikt450\\ikt450\\common\\datasets/food11/validation\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
       "               ToTensor()\n",
       "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "           )"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset Food11 from PATH_COMMON_DATASETS/food11\n",
    "# the folder structure is PATH_COMMON_DATASETS/food11/training and PATH_COMMON_DATASETS/food11/evaluation and PATH_COMMON_DATASETS/food11/validation\n",
    "#  and in these folders there are 11 subfolders with the class names\n",
    "#  and in these subfolders there are the images\n",
    "#  the dataset is loaded with the torchvision.datasets.ImageFolder function\n",
    "#  and the images are transformed to tensors and normalized\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), # random values from copilot\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # random values from copilot\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder(root=f\"{paths['PATH_COMMON_DATASETS']}/food11/training\" , transform=transform)\n",
    "train_dataset\n",
    "test_dataset = torchvision.datasets.ImageFolder(root=f\"{paths['PATH_COMMON_DATASETS']}/food11/evaluation\" , transform=transform)\n",
    "test_dataset\n",
    "val_dataset = torchvision.datasets.ImageFolder(root=f\"{paths['PATH_COMMON_DATASETS']}/food11/validation\" , transform=transform)\n",
    "val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 994,\n",
       " 1: 429,\n",
       " 2: 1500,\n",
       " 3: 986,\n",
       " 4: 848,\n",
       " 5: 1325,\n",
       " 6: 440,\n",
       " 7: 280,\n",
       " 8: 855,\n",
       " 9: 1500,\n",
       " 10: 709}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at label distribution\n",
    "# sum up the number of images in each class\n",
    "\n",
    "label_count = {}\n",
    "for i in train_dataset.targets:\n",
    "    if i in label_count:\n",
    "        label_count[i] += 1\n",
    "    else:\n",
    "        label_count[i] = 1\n",
    "label_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Function Definitions\n",
    "<!--\n",
    "- Define helper functions that will be used multiple times in the notebook.\n",
    "- Consider organizing these into separate sections (e.g., data processing functions, model evaluation functions).\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Distance Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1.1 Euclidian Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Data loading\n",
    "<!--\n",
    "- Load datasets from files or other sources.\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows\n",
      " Volume Serial Number is FA0F-7C2E\n",
      "\n",
      " Directory of C:\\Users\\jonin\\Documents\\ikt450\\ikt450\\common\\datasets\n",
      "\n",
      "10.09.2024  20:40    <DIR>          .\n",
      "28.08.2024  02:09    <DIR>          ..\n",
      "03.09.2024  21:22            19�488 ecoli.data\n",
      "03.09.2024  21:22             3�022 ecoli.names\n",
      "10.09.2024  20:40    <DIR>          food\n",
      "23.08.2024  18:45            23�278 pima-indians-diabetes.data.csv\n",
      "               3 File(s)         45�788 bytes\n",
      "               3 Dir(s)  297�591�087�104 bytes free\n"
     ]
    }
   ],
   "source": [
    "%ls {paths['PATH_COMMON_DATASETS']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Data inspection\n",
    "<!--\n",
    "- Preview the data (e.g., `head`, `describe`).\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Add code for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Data Cleaning\n",
    "<!--\n",
    "- Handle missing values, outliers, and inconsistencies.\n",
    "- Remove or impute missing data.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 NULL, NaN, Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Feature Engineering\n",
    "<!--\n",
    "- Create new features from existing data.\n",
    "- Normalize or standardize features.\n",
    "- Encode categorical variables.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1 Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.1.1 Feature Selection / Data Separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<br>\n",
    "<details>\n",
    "<summary>What does it?</summary>\n",
    "<br>\n",
    "This line removes the `` column from the DataFrame `df` and assigns the remaining columns to `X`.\n",
    "</details>\n",
    "<br>\n",
    "<details>\n",
    "<summary>Why do we do it?</summary>\n",
    "<br>\n",
    "We do this to separate the input features (which are stored in `X`) from the target variable (which will be stored in `y`). This separation is essential in supervised learning tasks where the goal is to predict the target variable based on the input features.\n",
    "</details>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='TODO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.1.2 Target Variable Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<br>\n",
    "<details>\n",
    "<summary>What does it?</summary>\n",
    "<br>\n",
    "This line selects the `` column from the DataFrame `df` and assigns it to `y`.\n",
    "</details>\n",
    "<br>\n",
    "<details>\n",
    "<summary>Why do we do it?</summary>\n",
    "<br>\n",
    "We do this to isolate the target variable, which represents the labels or outcomes that we aim to predict using the machine learning model.\n",
    "</details>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['TODO']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.1.3 Feature Scaling / Standardization / Z-score Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<br>\n",
    "<details>\n",
    "<summary>What does it?</summary>\n",
    "<br>\n",
    "This line standardizes the features in `X` by subtracting the mean of each feature and dividing by the standard deviation of that feature. This transforms the data so that each feature has a mean of 0 and a standard deviation of 1.\n",
    "</details>\n",
    "<br>\n",
    "<details>\n",
    "<summary>Why do we do it?</summary>\n",
    "<br>\n",
    "Standardization is crucial when using machine learning algorithms that rely on distance calculations (like K-Nearest Neighbors, SVM, or Neural Networks). Without standardization, features with larger scales could dominate the distance calculation, leading to biased model behavior. By standardizing, all features contribute equally to the model, regardless of their original scale.\n",
    "</details>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (X - X.mean()) / X.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Data Splitting\n",
    "<!--\n",
    "- Split data into training, validation, and test sets.\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=(1-SPLITRATIO), random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Model Selection\n",
    "<!--\n",
    "- Choose the model(s) to be trained (e.g., linear regression, decision trees, neural networks).\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Model Training\n",
    "<!--\n",
    "- Train the selected model(s) using the training data.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Model Evaluation\n",
    "<!--\n",
    "- Evaluate model performance on validation data.\n",
    "- Use appropriate metrics (e.g., accuracy, precision, recall, RMSE).\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Hyperparameter Tuning\n",
    "<!--\n",
    "- Fine-tune the model using techniques like Grid Search or Random Search.\n",
    "- Evaluate the impact of different hyperparameters.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Model Testing\n",
    "<!--\n",
    "- Evaluate the final model on the test dataset.\n",
    "- Ensure that the model generalizes well to unseen data.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Model Interpretation (Optional)\n",
    "<!--\n",
    "- Interpret the model results (e.g., feature importance, SHAP values).\n",
    "- Discuss the strengths and limitations of the model.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Make Predictions\n",
    "<!--\n",
    "- Use the trained model to make predictions on new/unseen data.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Save Model and Results\n",
    "<!--\n",
    "- Save the trained model to disk for future use.\n",
    "- Export prediction results for further analysis.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Documentation and Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Summary of Findings\n",
    "<!--\n",
    "- Summarize the results and findings of the analysis.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Next Steps\n",
    "<!--\n",
    "- Suggest further improvements, alternative models, or future work.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 References\n",
    "<!--\n",
    "- Cite any resources, papers, or documentation used.\n",
    "-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
