{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Notebook title -->\n",
    "# Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Notebook Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Task Description\n",
    "<!-- \n",
    "- A brief description of the problem you're solving with machine learning.\n",
    "- Define the objective (e.g., classification, regression, clustering, etc.).\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Useful Resources\n",
    "<!--\n",
    "- Links to relevant papers, articles, or documentation.\n",
    "- Description of the datasets (if external).\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1.1 Common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Datasets Kaggle](https://www.kaggle.com/datasets)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A vast repository of datasets across various domains provided by Kaggle, a platform for data science competitions.\n",
    "  \n",
    "* [Toy datasets from Sklearn](https://scikit-learn.org/stable/datasets/toy_dataset.html)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A collection of small datasets that come with the Scikit-learn library, useful for quick prototyping and testing algorithms.\n",
    "  \n",
    "* [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A widely-used repository for machine learning datasets, with a variety of real-world datasets available for research and experimentation.\n",
    "  \n",
    "* [Google Dataset Search](https://datasetsearch.research.google.com/)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A tool from Google that helps to find datasets stored across the web, with a focus on publicly available data.\n",
    "  \n",
    "* [AWS Public Datasets](https://registry.opendata.aws/)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A registry of publicly available datasets that can be analyzed on the cloud using Amazon Web Services (AWS).\n",
    "  \n",
    "* [Microsoft Azure Open Datasets](https://azure.microsoft.com/en-us/services/open-datasets/)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A collection of curated datasets from various domains, made available by Microsoft Azure for use in machine learning and analytics.\n",
    "  \n",
    "* [Awesome Public Datasets](https://github.com/awesomedata/awesome-public-datasets)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A GitHub repository that lists a wide variety of datasets across different domains, curated by the community.\n",
    "  \n",
    "* [Data.gov](https://www.data.gov/)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A portal to the US government's open data, offering access to a wide range of datasets from various federal agencies.\n",
    "  \n",
    "* [Google BigQuery Public Datasets](https://cloud.google.com/bigquery/public-data)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;Public datasets hosted by Google BigQuery, allowing for quick and powerful querying of large datasets in the cloud.\n",
    "  \n",
    "* [Papers with Code](https://paperswithcode.com/datasets)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A platform that links research papers with the corresponding code and datasets, helping researchers reproduce results and explore new data.\n",
    "  \n",
    "* [Zenodo](https://zenodo.org/)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;An open-access repository that allows researchers to share datasets, software, and other research outputs, often linked to academic publications.\n",
    "  \n",
    "* [The World Bank Open Data](https://data.worldbank.org/)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A comprehensive source of global development data, with datasets covering various economic and social indicators.\n",
    "  \n",
    "* [OpenML](https://www.openml.org/)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;An online platform for sharing datasets, machine learning experiments, and results, fostering collaboration in the ML community.\n",
    "  \n",
    "* [Stanford Large Network Dataset Collection (SNAP)](https://snap.stanford.edu/data/)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A collection of large-scale network datasets from Stanford University, useful for network analysis and graph-based machine learning.\n",
    "  \n",
    "* [KDnuggets Datasets](https://www.kdnuggets.com/datasets/index.html)  \n",
    "  &nbsp;&nbsp;&nbsp;&nbsp;A curated list of datasets for data mining and data science, compiled by the KDnuggets community.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1.2 Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [K-Nearest Neighbors on Kaggle](https://www.kaggle.com/code/mmdatainfo/k-nearest-neighbors)\n",
    "\n",
    "* [Complete Guide to K-Nearest-Neighbors](https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Imports\n",
    "<!--\n",
    "- Import necessary libraries (e.g., `numpy`, `pandas`, `matplotlib`, `scikit-learn`, etc.).\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ikt450.src.common_imports import *\n",
    "from ikt450.src.config import get_paths\n",
    "from ikt450.src.common_func import load_dataset, save_dataframe, ensure_dir_exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Global Variables\n",
    "<!--\n",
    "- Define global constants, paths, and configuration settings used throughout the notebook.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = get_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Split ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLITRATIO = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Function Definitions\n",
    "<!--\n",
    "- Define helper functions that will be used multiple times in the notebook.\n",
    "- Consider organizing these into separate sections (e.g., data processing functions, model evaluation functions).\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Distance Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1.1 Euclidian Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(one,two):\n",
    "    return np.linalg.norm(one-two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(X_train, y_train, X_test, k):\n",
    "    y_pred = []\n",
    "    for x_test in X_test:\n",
    "        # Calculate distances between x_test and all training samples\n",
    "        distances = [euclidean_distance(x_test, x_train) for x_train in X_train]\n",
    "        # Get the indices of k-nearest neighbors\n",
    "        k_indices = np.argsort(distances)[:k]\n",
    "        # Get the labels of the k-nearest neighbors\n",
    "        k_nearest_labels = [y_train[i] for i in k_indices]\n",
    "        # Determine the most common class label\n",
    "        most_common = Counter(k_nearest_labels).most_common(1)\n",
    "        y_pred.append(most_common[0][0])\n",
    "    return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. System Setup \n",
    "<!-- (Optional but recommended) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Styling\n",
    "<!--\n",
    "- Set up any visual styles (e.g., for plots).\n",
    "- Configure notebook display settings (e.g., `matplotlib` defaults, pandas display options).\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Environment Configuration\n",
    "<!--\n",
    "- Check system dependencies, versions, and ensure reproducibility (e.g., set random seeds).\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Data loading\n",
    "<!--\n",
    "- Load datasets from files or other sources.\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls {paths['PATH_COMMON_DATASETS']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{paths['PATH_COMMON_DATASETS']}/TODO\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Data inspection\n",
    "<!--\n",
    "- Preview the data (e.g., `head`, `describe`).\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Add code for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Data Cleaning\n",
    "<!--\n",
    "- Handle missing values, outliers, and inconsistencies.\n",
    "- Remove or impute missing data.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 NULL, NaN, Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Feature Engineering\n",
    "<!--\n",
    "- Create new features from existing data.\n",
    "- Normalize or standardize features.\n",
    "- Encode categorical variables.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1 Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.1.1 Feature Selection / Data Separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<br>\n",
    "<details>\n",
    "<summary>What does it?</summary>\n",
    "<br>\n",
    "This line removes the `` column from the DataFrame `df` and assigns the remaining columns to `X`.\n",
    "</details>\n",
    "<br>\n",
    "<details>\n",
    "<summary>Why do we do it?</summary>\n",
    "<br>\n",
    "We do this to separate the input features (which are stored in `X`) from the target variable (which will be stored in `y`). This separation is essential in supervised learning tasks where the goal is to predict the target variable based on the input features.\n",
    "</details>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='TODO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.1.2 Target Variable Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<br>\n",
    "<details>\n",
    "<summary>What does it?</summary>\n",
    "<br>\n",
    "This line selects the `` column from the DataFrame `df` and assigns it to `y`.\n",
    "</details>\n",
    "<br>\n",
    "<details>\n",
    "<summary>Why do we do it?</summary>\n",
    "<br>\n",
    "We do this to isolate the target variable, which represents the labels or outcomes that we aim to predict using the machine learning model.\n",
    "</details>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['TODO']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.1.3 Feature Scaling / Standardization / Z-score Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<br>\n",
    "<details>\n",
    "<summary>What does it?</summary>\n",
    "<br>\n",
    "This line standardizes the features in `X` by subtracting the mean of each feature and dividing by the standard deviation of that feature. This transforms the data so that each feature has a mean of 0 and a standard deviation of 1.\n",
    "</details>\n",
    "<br>\n",
    "<details>\n",
    "<summary>Why do we do it?</summary>\n",
    "<br>\n",
    "Standardization is crucial when using machine learning algorithms that rely on distance calculations (like K-Nearest Neighbors, SVM, or Neural Networks). Without standardization, features with larger scales could dominate the distance calculation, leading to biased model behavior. By standardizing, all features contribute equally to the model, regardless of their original scale.\n",
    "</details>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (X - X.mean()) / X.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Data Splitting\n",
    "<!--\n",
    "- Split data into training, validation, and test sets.\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=(1-SPLITRATIO), random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Model Selection\n",
    "<!--\n",
    "- Choose the model(s) to be trained (e.g., linear regression, decision trees, neural networks).\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Model Training\n",
    "<!--\n",
    "- Train the selected model(s) using the training data.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Model Evaluation\n",
    "<!--\n",
    "- Evaluate model performance on validation data.\n",
    "- Use appropriate metrics (e.g., accuracy, precision, recall, RMSE).\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Hyperparameter Tuning\n",
    "<!--\n",
    "- Fine-tune the model using techniques like Grid Search or Random Search.\n",
    "- Evaluate the impact of different hyperparameters.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Model Testing\n",
    "<!--\n",
    "- Evaluate the final model on the test dataset.\n",
    "- Ensure that the model generalizes well to unseen data.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Model Interpretation (Optional)\n",
    "<!--\n",
    "- Interpret the model results (e.g., feature importance, SHAP values).\n",
    "- Discuss the strengths and limitations of the model.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Make Predictions\n",
    "<!--\n",
    "- Use the trained model to make predictions on new/unseen data.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Save Model and Results\n",
    "<!--\n",
    "- Save the trained model to disk for future use.\n",
    "- Export prediction results for further analysis.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Documentation and Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Summary of Findings\n",
    "<!--\n",
    "- Summarize the results and findings of the analysis.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Next Steps\n",
    "<!--\n",
    "- Suggest further improvements, alternative models, or future work.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 References\n",
    "<!--\n",
    "- Cite any resources, papers, or documentation used.\n",
    "-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
